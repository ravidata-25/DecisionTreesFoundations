{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravidata-25/DecisionTreesFoundations/blob/main/Aritificial_Neural_Networks_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are Artificial Neural Networks (ANNs)?**\n",
        "\n",
        "You can introduce ANNs using an analogy to the human brain.\n",
        "The Neuron: At its core, the brain is made of billions of neurons that are connected to each other. An individual neuron is not very smart, but when connected, they form a powerful network. An ANN is a computational model inspired by this structure. The \"neurons\" in an ANN are called nodes.\n",
        "\n",
        "\n",
        "Layers: These nodes are organized into layers. Every ANN has at least two:\n",
        "\n",
        "An Input Layer: This is where the data comes in. You have one node for each feature in your dataset (e.g., one for CreditScore, one for Age, one for Balance, etc.).\n",
        "\n",
        "An Output Layer: This layer produces the final prediction. For a binary classification problem like ours (\"Will the customer exit? Yes/No\"), this layer often has just one node that outputs a probability (e.g., a 78% chance the customer will leave).\n",
        "\n",
        "Hidden Layers: Between the input and output layers, we can have one or more hidden layers. These are the \"processing\" centers of the network. It's in these layers that the ANN learns to identify complex patterns and non-linear relationships in the data.\n",
        "\n",
        "The network learns how to combine the input features in interesting ways to make a better prediction. A network with many hidden layers is called a \"deep\" neural network, which is where the term \"deep learning\" comes from.\n",
        "\n",
        "Connections and Weights: Every node in one layer is connected to nodes in the next layer. Each connection has a weight associated with it. This weight represents the strength or importance of that connection. During training, the network's main goal is to find the optimal set of weights that produces the most accurate predictions.\n",
        "\n",
        "\n",
        "Activation Functions: Inside each node (except in the input layer), an activation function is applied. This function decides whether a neuron should be \"activated\" or not, based on the weighted sum of inputs it receives. It introduces non-linearity into the model, which is crucial. Without it, the ANN would just be a complex linear regression model, unable to capture intricate patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "b6OgactyQQdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why and When Do We Use ANNs?**\n",
        "\n",
        "This is a key question for your students. ANNs are not always the best tool for the job.\n",
        "\n",
        "When to Use ANNs:\n",
        "\n",
        "Complex, Non-Linear Problems: ANNs excel when the relationship between the input features and the output is complex and not easily captured by simpler models like linear or logistic regression. Customer churn is a perfect example; the decision to leave a bank is likely a combination of many subtle factors, not a simple straight-line relationship.\n",
        "\n",
        "\n",
        "Large Datasets: Neural networks are data-hungry. They need a lot of examples to learn the optimal weights. With thousands or millions of data points (like in our dataset of 10,000 customers), they can uncover patterns that would be invisible in smaller datasets.\n",
        "\n",
        "\n",
        "High-Dimensional Data: They work very well on problems with many input features, such as image recognition (where every pixel is a feature) or natural language processing.\n",
        "\n",
        "\n",
        "When Predictive Performance is Paramount: If your primary goal is to get the most accurate prediction possible, and you care less about understanding the why behind the prediction, ANNs are a top choice.\n",
        "\n",
        "\n",
        "###  **When to Consider Alternatives: **\n",
        "\n",
        "Small Datasets: On small datasets, ANNs are prone to overfitting—essentially memorizing the training data instead of learning a general pattern. Simpler models like Logistic Regression, Decision Trees, or SVMs often perform better and are less computationally expensive.\n",
        "\n",
        "Need for Interpretability: ANNs are often called \"black boxes.\" It's very difficult to look at the millions of weights and understand exactly why the model made a specific prediction.\n",
        "\n",
        " If you need to explain the decision-making process to a regulator or a business stakeholder (e.g., \"This customer was denied a loan because their debt-to-income ratio was too high\"), a Decision Tree or Logistic Regression model would be a much better choice."
      ],
      "metadata": {
        "id": "Y7zF0Om6Ra6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzNTsnFPQPTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdkb-3rgI7uG"
      },
      "outputs": [],
      "source": [
        "#----------------------- Artificial Neural Network for classification --------------------#\n",
        "#importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.compose import ColumnTransformer\n",
        "# What it is: This is a very useful tool for applying different preprocessing steps to different columns of the data.\n",
        "# Why we need it: We want to OneHotEncode the 'Geography' column but leave the other columns (like 'Age' and 'Balance') alone.\n",
        "#  ColumnTransformer lets us do exactly that in one clean step.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------- Data Pre-processing ----------------------#\n",
        "# Checking the tensorflow version\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xi3oSbjI87e",
        "outputId": "694c79ce-2c2c-4e04-c3b6-7b932091fde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "bank_data = pd.read_csv(\"/content/Artificial_Neural_Network_Case_Study_data.csv\")"
      ],
      "metadata": {
        "id": "AGoApFQ6I89v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bank_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "7L2PvfnNJUdV",
        "outputId": "aec5a9c0-4e2e-486b-c6e5-e0e47922afed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
              "0          1    15634602  Hargrave          619    France  Female   42   \n",
              "1          2    15647311      Hill          608     Spain  Female   41   \n",
              "2          3    15619304      Onio          502    France  Female   42   \n",
              "3          4    15701354      Boni          699    France  Female   39   \n",
              "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
              "\n",
              "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
              "0       2       0.00              1          1               1   \n",
              "1       1   83807.86              1          0               1   \n",
              "2       8  159660.80              3          1               0   \n",
              "3       1       0.00              2          0               0   \n",
              "4       2  125510.82              1          1               1   \n",
              "\n",
              "   EstimatedSalary  Exited  \n",
              "0        101348.88       1  \n",
              "1        112542.58       0  \n",
              "2        113931.57       1  \n",
              "3         93826.63       0  \n",
              "4         79084.10       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aab169e2-c452-4bb0-a2be-f55cd93acf51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aab169e2-c452-4bb0-a2be-f55cd93acf51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aab169e2-c452-4bb0-a2be-f55cd93acf51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aab169e2-c452-4bb0-a2be-f55cd93acf51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cc42e766-9d7a-41f5-8a70-4c521ace449a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc42e766-9d7a-41f5-8a70-4c521ace449a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cc42e766-9d7a-41f5-8a70-4c521ace449a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "bank_data",
              "summary": "{\n  \"name\": \"bank_data\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"RowNumber\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2886,\n        \"min\": 1,\n        \"max\": 10000,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          6253,\n          4685,\n          1732\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CustomerId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 71936,\n        \"min\": 15565701,\n        \"max\": 15815690,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          15687492,\n          15736963,\n          15721730\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Surname\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2932,\n        \"samples\": [\n          \"McGuirk\",\n          \"Torkelson\",\n          \"Rapuluchukwu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CreditScore\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 350,\n        \"max\": 850,\n        \"num_unique_values\": 460,\n        \"samples\": [\n          754,\n          533,\n          744\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Geography\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"France\",\n          \"Spain\",\n          \"Germany\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Male\",\n          \"Female\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 18,\n        \"max\": 92,\n        \"num_unique_values\": 70,\n        \"samples\": [\n          61,\n          42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tenure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 10,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          6,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Balance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62397.40520238623,\n        \"min\": 0.0,\n        \"max\": 250898.09,\n        \"num_unique_values\": 6382,\n        \"samples\": [\n          117707.18,\n          133050.97\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NumOfProducts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HasCrCard\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IsActiveMember\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EstimatedSalary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57510.49281769822,\n        \"min\": 11.58,\n        \"max\": 199992.48,\n        \"num_unique_values\": 9999,\n        \"samples\": [\n          100809.99,\n          95273.73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Exited\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bank_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-UpKHRnI9AO",
        "outputId": "fb2c8f33-b8f9-4164-91d8-417789484fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bank_data.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "0iQ9sIdnI9Ca",
        "outputId": "158412a1-c0dd-4e20-b423-9d2a909786ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RowNumber          0\n",
              "CustomerId         0\n",
              "Surname            0\n",
              "CreditScore        0\n",
              "Geography          0\n",
              "Gender             0\n",
              "Age                0\n",
              "Tenure             0\n",
              "Balance            0\n",
              "NumOfProducts      0\n",
              "HasCrCard          0\n",
              "IsActiveMember     0\n",
              "EstimatedSalary    0\n",
              "Exited             0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>RowNumber</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CustomerId</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Surname</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CreditScore</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Geography</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Gender</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tenure</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Balance</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NumOfProducts</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HasCrCard</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IsActiveMember</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Exited</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking  all rows and all columns in the data except the last column as X (feature matrix)\n",
        "#the row numbers and customer id's are not necessary for the modelling so we get rid of and start with credit score\n",
        "X = bank_data.iloc[:,3:-1].values\n",
        "print(\"Independent variables are:\", X)\n",
        "\n",
        "'''\n",
        "bank_data.iloc is how we select data in Pandas by its numerical position.\n",
        "The [:, 3:-1] part is the key. The first colon : means 'give me all the rows'.\n",
        "The 3:-1 means 'start at the 4th column (index 3) and go up to, but not including, the last column'.\n",
        "Why start at column 3? Because RowNumber, CustomerId, and Surname are just identifiers.\n",
        "They don't have any predictive power, so we exclude them.\n",
        "The .values at the end converts our data from a Pandas table into a NumPy array, which is the format our model requires.\n",
        "\n",
        "'''\n",
        "\n",
        "#taking all rows but only the last column as Y(dependent variable)\n",
        "y = bank_data.iloc[:, -1].values\n",
        "print(\"Dependent variable is:\", y)\n",
        "\n",
        "\n",
        "'''\n",
        "This is simpler. We're telling it to take all rows (:) and only the very last column (-1),\n",
        "which is our 'Exited' column. This column contains the 'answers' our model needs to learn from.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "5V-iHHPKI9FA",
        "outputId": "7df74d80-deb1-484b-faed-0afd3abb19c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Independent variables are: [[619 'France' 'Female' ... 1 1 101348.88]\n",
            " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
            " [502 'France' 'Female' ... 1 0 113931.57]\n",
            " ...\n",
            " [709 'France' 'Female' ... 0 1 42085.58]\n",
            " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
            " [792 'France' 'Female' ... 1 0 38190.78]]\n",
            "Dependent variable is: [1 0 1 ... 1 1 0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThis is simpler. We're telling it to take all rows (:) and only the very last column (-1),\\nwhich is our 'Exited' column. This column contains the 'answers' our model needs to learn from.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming the gender variable, labels are chosen randomly\n",
        "le = LabelEncoder()\n",
        "X[:,2] = le.fit_transform(X[:,2])\n",
        "print(X)\n",
        "\n",
        "'''\n",
        "le = LabelEncoder(): First, we create an instance of the LabelEncoder object, which we'll call le.\n",
        "X[:,2] = le.fit_transform(X[:,2]): This is the main action.\n",
        "X[:,2] selects all rows (:) but only the third column (2), which is our 'Gender' column.\n",
        "The fit_transform method does two things at once:\n",
        "fit: It looks at the column and learns all the unique categories ('Female' and 'Male').\n",
        "transform: It then converts each of those categories into an integer. For example, it will assign 'Female' to 0 and 'Male' to 1.\n",
        "Finally, we replace the original 'Gender' column with these new numerical values.\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "ZhghlUplI9HQ",
        "outputId": "80961f25-29bf-48ac-d8aa-7cee1cddf66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[619 'France' 0 ... 1 1 101348.88]\n",
            " [608 'Spain' 0 ... 0 1 112542.58]\n",
            " [502 'France' 0 ... 1 0 113931.57]\n",
            " ...\n",
            " [709 'France' 0 ... 0 1 42085.58]\n",
            " [772 'Germany' 1 ... 1 0 92888.52]\n",
            " [792 'France' 0 ... 1 0 38190.78]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nle = LabelEncoder(): First, we create an instance of the LabelEncoder object, which we'll call le.\\nX[:,2] = le.fit_transform(X[:,2]): This is the main action.\\nX[:,2] selects all rows (:) but only the third column (2), which is our 'Gender' column.\\nThe fit_transform method does two things at once:\\nfit: It looks at the column and learns all the unique categories ('Female' and 'Male').\\ntransform: It then converts each of those categories into an integer. For example, it will assign 'Female' to 0 and 'Male' to 1.\\nFinally, we replace the original 'Gender' column with these new numerical values.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming the geography column variable, labels are chosen randomly, the ct asks for argument [1] the index of the target vb\n",
        "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(),[1])], remainder = 'passthrough')\n",
        "X = np.array(ct.fit_transform(X))\n",
        "print(X)\n",
        "\n",
        "\n",
        "'''\n",
        "We could use LabelEncoder again, which would turn them into 0, 1, and 2. But this creates a subtle problem.\n",
        "It implies an ordinal relationship—that Germany (2) is somehow 'greater' than Spain (1), which is 'greater' than France (0).\n",
        "Our model might mistakenly learn this non-existent order, which is bad.\n",
        "\n",
        "\n",
        "To avoid this, we use a better technique for columns with more than two categories: One-Hot Encoding.\n",
        "This creates new binary columns for each category.\n",
        "ct = ColumnTransformer(...): We use ColumnTransformer to apply this change only to the geography column while leaving all\n",
        "other columns untouched.\n",
        "transformers = [('encoder', OneHotEncoder(), [1])]: This is the core instruction. We're telling it:\n",
        "Apply an OneHotEncoder()...\n",
        "...to the column at index 1 (which is our 'Geography' column).\n",
        "remainder = 'passthrough': This is very important. It tells the ColumnTransformer to just let all the other columns\n",
        "(CreditScore, Age, etc.) pass through without any changes.\n",
        "X = np.array(ct.fit_transform(X)): We apply this transformation to our entire feature matrix X.\n",
        "The 'Geography' column is replaced by three new columns at the very beginning of our dataset, representing France, Germany, and Spain.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "rIQ5ucqjJAQa",
        "outputId": "442186ec-d638-46dc-ed54-5eb044a9c38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
            " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
            " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
            " ...\n",
            " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
            " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
            " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nWe could use LabelEncoder again, which would turn them into 0, 1, and 2. But this creates a subtle problem.\\nIt implies an ordinal relationship—that Germany (2) is somehow 'greater' than Spain (1), which is 'greater' than France (0).\\nOur model might mistakenly learn this non-existent order, which is bad.\\n\\n\\nTo avoid this, we use a better technique for columns with more than two categories: One-Hot Encoding.\\nThis creates new binary columns for each category.\\nct = ColumnTransformer(...): We use ColumnTransformer to apply this change only to the geography column while leaving all\\nother columns untouched.\\ntransformers = [('encoder', OneHotEncoder(), [1])]: This is the core instruction. We're telling it:\\nApply an OneHotEncoder()...\\n...to the column at index 1 (which is our 'Geography' column).\\nremainder = 'passthrough': This is very important. It tells the ColumnTransformer to just let all the other columns\\n(CreditScore, Age, etc.) pass through without any changes.\\nX = np.array(ct.fit_transform(X)): We apply this transformation to our entire feature matrix X.\\nThe 'Geography' column is replaced by three new columns at the very beginning of our dataset, representing France, Germany, and Spain.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "#printing the dimensions of each of those snapshots to see amount of rows and columns i each of them\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)\n",
        "\n",
        "\n",
        "'''\n",
        "Think of it like preparing for an exam. The train set is your textbook and practice problems.\n",
        "The test set is the final, unseen exam. Your grade on the final exam is the true measure of what you've learned.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(...): This function takes our full dataset (X and y) and shuffles\n",
        "it randomly before splitting it into four new sets:\n",
        "X_train: The features we will use to teach our model.\n",
        "y_train: The corresponding correct answers for X_train.\n",
        "X_test: The features we will use to evaluate our model's performance on unseen data.\n",
        "y_test: The corresponding correct answers for X_test, which we'll use to grade the model's predictions.\n",
        "test_size = 0.2: This parameter tells the function to hold back 20% of the data for the test set.\n",
        " This means 80% will be used for training. This is a common and good starting point for the split ratio.\n",
        "\n",
        "random_state = 0: This is for reproducibility. By setting a random_state, we ensure that every time we run this code,\n",
        " the data is shuffled and split in the exact same way. This is crucial for getting consistent results when we are developing and comparing models.\n",
        "The print statements confirm the split.\n",
        "\n",
        "We started with 10,000 customers. Now we have 8,000 for training ((8000, 12)) and 2,000 for testing ((2000, 12)),\n",
        "just as we specified.\"\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "f-Syb1GbJAS3",
        "outputId": "8bea6a80-4a0f-4ad2-8f31-de3e84f58203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8000, 12) (2000, 12)\n",
            "(8000,) (2000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThink of it like preparing for an exam. The train set is your textbook and practice problems.\\nThe test set is the final, unseen exam. Your grade on the final exam is the true measure of what you\\'ve learned.\\n\\nX_train, X_test, y_train, y_test = train_test_split(...): This function takes our full dataset (X and y) and shuffles\\nit randomly before splitting it into four new sets:\\nX_train: The features we will use to teach our model.\\ny_train: The corresponding correct answers for X_train.\\nX_test: The features we will use to evaluate our model\\'s performance on unseen data.\\ny_test: The corresponding correct answers for X_test, which we\\'ll use to grade the model\\'s predictions.\\ntest_size = 0.2: This parameter tells the function to hold back 20% of the data for the test set.\\n This means 80% will be used for training. This is a common and good starting point for the split ratio.\\n\\nrandom_state = 0: This is for reproducibility. By setting a random_state, we ensure that every time we run this code,\\n the data is shuffled and split in the exact same way. This is crucial for getting consistent results when we are developing and comparing models.\\nThe print statements confirm the split.\\n\\nWe started with 10,000 customers. Now we have 8,000 for training ((8000, 12)) and 2,000 for testing ((2000, 12)),\\njust as we specified.\"\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Scaling/normalization of the features that will go to the NN\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "'''\n",
        "For a neural network, this massive difference in scale is a problem.\n",
        "The features with larger values can dominate the learning process, causing the model to train slowly or inefficiently.\n",
        "\n",
        "We solve this with Feature Scaling. The goal is to put all our features onto a similar scale, so they all contribute fairly.\n",
        "\n",
        "sc = StandardScaler(): We create an instance of the StandardScaler.\n",
        "This particular scaler transforms the data so that each feature has a mean of 0 and a standard deviation of 1.\n",
        "X_train = sc.fit_transform(X_train): This is a two-step process on our training data.\n",
        "\n",
        "fit: The scaler calculates the mean and standard deviation for each column in X_train.\n",
        "It learns the scale of our training data.\n",
        "\n",
        "transform: It then uses these calculated values to scale the training data.\n",
        "\n",
        "\n",
        "X_test = sc.transform(X_test): This is a very important distinction. For the test set, we only use transform.\n",
        "Why not fit_transform again? Because we must use the same scaling parameters (the same mean and standard deviation) that we learned from the training set. The test set represents new, unseen data, and we must process it in the exact same way we processed our training data. We are pretending we don't know the distribution of the test set, just like we wouldn't in the real world.\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "9K8ol5cUJAVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "9da48c78-f1a7-4a4a-c767-9fafc5aaa849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nFor a neural network, this massive difference in scale is a problem.\\nThe features with larger values can dominate the learning process, causing the model to train slowly or inefficiently.\\n\\nWe solve this with Feature Scaling. The goal is to put all our features onto a similar scale, so they all contribute fairly.\\n\\nsc = StandardScaler(): We create an instance of the StandardScaler.\\nThis particular scaler transforms the data so that each feature has a mean of 0 and a standard deviation of 1.\\nX_train = sc.fit_transform(X_train): This is a two-step process on our training data.\\n\\nfit: The scaler calculates the mean and standard deviation for each column in X_train.\\nIt learns the scale of our training data.\\n\\ntransform: It then uses these calculated values to scale the training data.\\n\\n\\nX_test = sc.transform(X_test): This is a very important distinction. For the test set, we only use transform.\\nWhy not fit_transform again? Because we must use the same scaling parameters (the same mean and standard deviation) that we learned from the training set. The test set represents new, unseen data, and we must process it in the exact same way we processed our training data. We are pretending we don't know the distribution of the test set, just like we wouldn't in the real world.\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-foWB0qGTHS6",
        "outputId": "f565979e-41a3-4248-a7dc-bb96d63ce1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.01460667, -0.5698444 ,  1.74309049, ...,  0.64259497,\n",
              "        -1.03227043,  1.10643166],\n",
              "       [-1.01460667,  1.75486502, -0.57369368, ...,  0.64259497,\n",
              "         0.9687384 , -0.74866447],\n",
              "       [ 0.98560362, -0.5698444 , -0.57369368, ...,  0.64259497,\n",
              "        -1.03227043,  1.48533467],\n",
              "       ...,\n",
              "       [ 0.98560362, -0.5698444 , -0.57369368, ...,  0.64259497,\n",
              "        -1.03227043,  1.41231994],\n",
              "       [-1.01460667, -0.5698444 ,  1.74309049, ...,  0.64259497,\n",
              "         0.9687384 ,  0.84432121],\n",
              "       [-1.01460667,  1.75486502, -0.57369368, ...,  0.64259497,\n",
              "        -1.03227043,  0.32472465]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------- Building the model -----------------------#\n",
        "\n",
        "# Initializing the ANN by calling the Sequential class fromm keras of Tensorflow\n",
        "ann = tf.keras.models.Sequential()\n",
        "\n",
        "\n",
        "'''\n",
        "Think of this first line as laying the foundation for a building.\n",
        "We're creating an empty container that we will add layers to, one after the other.\n",
        "\n",
        "ann = tf.keras.models.Sequential():\n",
        "tf.keras is the user-friendly API within TensorFlow that we use to build models.\n",
        "\n",
        "A Sequential model is the most common type of model. It simply means we are building our network layer by layer, in a linear stack.\n",
        "We will add the input layer, then a hidden layer, then another hidden layer, and finally the output layer.\n",
        "We are assigning this empty model object to the variable ann (short for Artificial Neural Network).\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "OCW_3jiiJAXR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c2f286b2-ddbe-4ce5-e170-f5cec32b7dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThink of this first line as laying the foundation for a building.\\nWe're creating an empty container that we will add layers to, one after the other.\\n\\nann = tf.keras.models.Sequential():\\ntf.keras is the user-friendly API within TensorFlow that we use to build models.\\n\\nA Sequential model is the most common type of model. It simply means we are building our network layer by layer, in a linear stack.\\nWe will add the input layer, then a hidden layer, then another hidden layer, and finally the output layer.\\nWe are assigning this empty model object to the variable ann (short for Artificial Neural Network).\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztNy6UBITrFn",
        "outputId": "34ba4386-7eb0-4a76-e7be-142448bfcf87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Sequential name=sequential, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding \"fully connected\" INPUT layer to the Sequential ANN by calling Dense class\n",
        "# Number of Units = 6 and Activation Function = Rectifier\n",
        "ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))\n",
        "\n",
        "'''\n",
        "\"Now we add our first 'floor' of neurons to our empty model. This will be our first hidden layer.\n",
        "ann.add(...): This is how we add a new layer to our Sequential model.\n",
        "\n",
        "tf.keras.layers.Dense(...): A Dense layer is the most basic and common type of layer.\n",
        "'Dense' simply means that every neuron in this layer is connected to every neuron in the previous layer.\n",
        "\n",
        "Let's look at the two key parameters we've set:\n",
        "units = 6: This defines the number of neurons in this layer. We've chosen 6.\n",
        "Why 6? The number of neurons in a hidden layer is a hyperparameter.\n",
        "\n",
        "There's no single perfect answer; it's often found through experimentation.\n",
        "A common rule of thumb is to choose a number somewhere between the number of input features (we have 12) and\n",
        "the number of output neurons (we'll have 1). Starting with 6 is a reasonable choice.\n",
        "activation = 'relu': This is the activation function for the neurons in this layer.\n",
        "\n",
        "relu stands for Rectified Linear Unit. It's the most popular activation function for hidden layers.\n",
        "What it does: It's a very simple function. If the input to the neuron is negative, it outputs 0.\n",
        "If the input is positive, it outputs the input value itself.\n",
        "\n",
        "Why use it? It's computationally efficient and helps the network learn complex patterns\n",
        "without running into certain mathematical problems that older activation functions had.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "AZi-ag9mJAZ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "cc5e5549-8100-449c-ec66-5ac0bf14a373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"Now we add our first \\'floor\\' of neurons to our empty model. This will be our first hidden layer.\\nann.add(...): This is how we add a new layer to our Sequential model.\\n\\ntf.keras.layers.Dense(...): A Dense layer is the most basic and common type of layer.\\n\\'Dense\\' simply means that every neuron in this layer is connected to every neuron in the previous layer.\\n\\nLet\\'s look at the two key parameters we\\'ve set:\\nunits = 6: This defines the number of neurons in this layer. We\\'ve chosen 6.\\nWhy 6? The number of neurons in a hidden layer is a hyperparameter.\\n\\nThere\\'s no single perfect answer; it\\'s often found through experimentation.\\nA common rule of thumb is to choose a number somewhere between the number of input features (we have 12) and \\nthe number of output neurons (we\\'ll have 1). Starting with 6 is a reasonable choice.\\nactivation = \\'relu\\': This is the activation function for the neurons in this layer.\\n\\nrelu stands for Rectified Linear Unit. It\\'s the most popular activation function for hidden layers.\\nWhat it does: It\\'s a very simple function. If the input to the neuron is negative, it outputs 0.\\nIf the input is positive, it outputs the input value itself.\\n\\nWhy use it? It\\'s computationally efficient and helps the network learn complex patterns\\nwithout running into certain mathematical problems that older activation functions had.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------------------------------------\n",
        "# Adding \"fully connected\" SECOND layer to the Sequential ANN by calling Dense class\n",
        "#----------------------------------------------------------------------------------\n",
        "# Number of Units = 6 and Activation Function = Rectifier\n",
        "ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))\n",
        "\n",
        "\n",
        "'''\n",
        "We're now adding another floor to our building—a second hidden layer. This is what makes our network 'deeper'\n",
        ".\n",
        "The code is identical to the previous step, and for good reason. We are adding another layer with the same structure.\n",
        "ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu')):\n",
        "\n",
        "Again, we use a Dense layer, meaning every neuron in this new layer will be connected to all 6 neurons from the first hidden layer.\n",
        "We are again choosing 6 units and the relu activation function. Keeping the architecture symmetrical like this (6 -> 6)\n",
        " is a common practice, though not a strict rule.\n",
        "\n",
        "Why add a second layer?\n",
        "\n",
        "By adding more layers, we allow the network to learn more complex and abstract patterns from the data. The first layer\n",
        "might learn simple relationships between features, and the second layer can then learn patterns from the outputs of the first layer.\n",
        "This hierarchical learning is what gives deep learning its power.\n",
        "Our network structure so far is: Input Layer -> Hidden Layer 1 (6 neurons) -> Hidden Layer 2 (6 neurons).\"\n",
        "'''"
      ],
      "metadata": {
        "id": "1MI-a0UGJo1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "c0046dbd-2ce3-48c6-80c0-8c7bb32a53c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe\\'re now adding another floor to our building—a second hidden layer. This is what makes our network \\'deeper\\'\\n.\\nThe code is identical to the previous step, and for good reason. We are adding another layer with the same structure.\\nann.add(tf.keras.layers.Dense(units = 6, activation = \\'relu\\')):\\n\\nAgain, we use a Dense layer, meaning every neuron in this new layer will be connected to all 6 neurons from the first hidden layer.\\nWe are again choosing 6 units and the relu activation function. Keeping the architecture symmetrical like this (6 -> 6)\\n is a common practice, though not a strict rule.\\n\\nWhy add a second layer?\\n\\nBy adding more layers, we allow the network to learn more complex and abstract patterns from the data. The first layer \\nmight learn simple relationships between features, and the second layer can then learn patterns from the outputs of the first layer. \\nThis hierarchical learning is what gives deep learning its power.\\nOur network structure so far is: Input Layer -> Hidden Layer 1 (6 neurons) -> Hidden Layer 2 (6 neurons).\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1CSR3J0sZbrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------------------------------------\n",
        "# Adding \"fully connected\" OUTPUT layer to the Sequential ANN by calling Dense class\n",
        "#----------------------------------------------------------------------------------\n",
        "# Number of Units = 1 and Activation Function = Sigmoid\n",
        "ann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "'''\n",
        "\"We've built the processing floors of our network; now we need the top floor—the output layer.\n",
        "This layer is responsible for producing the final prediction.\n",
        "\n",
        "The structure of this layer is very specific to the problem we're trying to solve.\n",
        "\n",
        "units = 1: We set the number of neurons to 1.\n",
        "\n",
        "Why one? Because we are solving a binary classification problem.\n",
        " The final output we want is a single number: the probability that the customer will exit.\n",
        " A single neuron is all we need to output that one number.\n",
        "\n",
        "activation = 'sigmoid': This is a different activation function, and its choice is deliberate.\n",
        "\n",
        "What it does: The sigmoid function is a special S-shaped curve that squashes any input value into a range between 0 and 1.\n",
        "\n",
        "Why use it here? This is perfect for our output. A value of 0.8 can be interpreted as an 80% probability of churn.\n",
        "A value of 0.1 means a 10% probability.\n",
        "It's the ideal activation function for binary classification because it directly gives us the probability we need\n",
        "to make our final decision.\n",
        "\n",
        "So, with this step, our network architecture is complete. It looks like this:\n",
        "\n",
        "Input Layer -> Hidden Layer 1 (6 relu neurons) -> Hidden Layer 2 (6 relu neurons) -> Output Layer (1 sigmoid neuron)\"\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "SfK4-ZpLJo4J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "4cd99c16-08cd-4816-be76-e945bf8323a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"We\\'ve built the processing floors of our network; now we need the top floor—the output layer.\\nThis layer is responsible for producing the final prediction.\\n\\nThe structure of this layer is very specific to the problem we\\'re trying to solve.\\n\\nunits = 1: We set the number of neurons to 1.\\n\\nWhy one? Because we are solving a binary classification problem.\\n The final output we want is a single number: the probability that the customer will exit.\\n A single neuron is all we need to output that one number.\\n\\nactivation = \\'sigmoid\\': This is a different activation function, and its choice is deliberate.\\n\\nWhat it does: The sigmoid function is a special S-shaped curve that squashes any input value into a range between 0 and 1.\\n\\nWhy use it here? This is perfect for our output. A value of 0.8 can be interpreted as an 80% probability of churn.\\nA value of 0.1 means a 10% probability.\\nIt\\'s the ideal activation function for binary classification because it directly gives us the probability we need\\nto make our final decision.\\n\\nSo, with this step, our network architecture is complete. It looks like this:\\n\\nInput Layer -> Hidden Layer 1 (6 relu neurons) -> Hidden Layer 2 (6 relu neurons) -> Output Layer (1 sigmoid neuron)\"\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------- Training the model -----------------------#\n",
        "# Compiling the ANN\n",
        "# Type of Optimizer = Adam Optimizer, Loss Function =  crossentropy for binary dependent variable, and Optimization is done w.r.t. accuracy\n",
        "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "'''\n",
        "\"We've designed our neural network's architecture, but it doesn't know how to learn yet.\n",
        "\n",
        "The compile step is where we give it the tools and instructions for the learning process. We need to define three key things:\n",
        "\n",
        "optimizer = 'adam': The optimizer is the engine that drives the learning.\n",
        "\n",
        "What it does: During training, the optimizer's job is to adjust the network's weights in a way that minimizes the error.\n",
        "It's the algorithm that implements gradient descent.\n",
        "\n",
        "Why 'adam'? The Adam optimizer is a highly effective, popular, and robust choice.\n",
        "It's efficient and generally works well across a wide range of problems, making it an excellent default choice.\n",
        "\n",
        "loss = 'binary_crossentropy': The loss function is how we measure the model's error.\n",
        "\n",
        "What it does: For each prediction, the loss function calculates a 'penalty' or 'cost' based on how far\n",
        "the prediction was from the actual true value. The goal of training is to make the total loss as low as possible.\n",
        "\n",
        "Why 'binary_crossentropy'? This is the standard, mathematically-optimized loss function for a binary (two-class)\n",
        "classification problem, especially when the output of our model is a probability from a sigmoid function.\n",
        "\n",
        "It penalizes the model heavily for being confident and wrong.\n",
        "\n",
        "metrics = ['accuracy']: Metrics are used to monitor the training and testing steps.\n",
        "\n",
        "\n",
        "What it does: While the loss function is what the optimizer tries to minimize, it's not always easy for humans to interpret.\n",
        "Accuracy, on the other hand, is very intuitive: \"What percentage of predictions are correct?\"\n",
        "Why use it? We tell the model that during training, in addition to calculating the loss, we also want it to keep track of the accuracy. This helps us gauge the model's performance in a way that's easy to understand.\n",
        "In summary, we've just told our model: 'Your goal is to minimize the binary_crossentropy loss. The tool you will use to do this is the adam optimizer. And along the way, please report back on your accuracy so we can see how you're doing.'\"\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "x6swf3lLJo62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b5b9b046-b520-4d66-c17f-e28bacfc5286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"We\\'ve designed our neural network\\'s architecture, but it doesn\\'t know how to learn yet.\\n\\nThe compile step is where we give it the tools and instructions for the learning process. We need to define three key things:\\n\\noptimizer = \\'adam\\': The optimizer is the engine that drives the learning.\\n\\nWhat it does: During training, the optimizer\\'s job is to adjust the network\\'s weights in a way that minimizes the error.\\nIt\\'s the algorithm that implements gradient descent.\\n\\nWhy \\'adam\\'? The Adam optimizer is a highly effective, popular, and robust choice.\\nIt\\'s efficient and generally works well across a wide range of problems, making it an excellent default choice.\\n\\nloss = \\'binary_crossentropy\\': The loss function is how we measure the model\\'s error.\\n\\nWhat it does: For each prediction, the loss function calculates a \\'penalty\\' or \\'cost\\' based on how far\\nthe prediction was from the actual true value. The goal of training is to make the total loss as low as possible.\\n\\nWhy \\'binary_crossentropy\\'? This is the standard, mathematically-optimized loss function for a binary (two-class)\\nclassification problem, especially when the output of our model is a probability from a sigmoid function.\\n\\nIt penalizes the model heavily for being confident and wrong.\\n\\nmetrics = [\\'accuracy\\']: Metrics are used to monitor the training and testing steps.\\n\\n\\nWhat it does: While the loss function is what the optimizer tries to minimize, it\\'s not always easy for humans to interpret. \\nAccuracy, on the other hand, is very intuitive: \"What percentage of predictions are correct?\"\\nWhy use it? We tell the model that during training, in addition to calculating the loss, we also want it to keep track of the accuracy. This helps us gauge the model\\'s performance in a way that\\'s easy to understand.\\nIn summary, we\\'ve just told our model: \\'Your goal is to minimize the binary_crossentropy loss. The tool you will use to do this is the adam optimizer. And along the way, please report back on your accuracy so we can see how you\\'re doing.\\'\"\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the ANN model on training set  (fit method always the same)\n",
        "# batch_size = 32, the default value, number of epochs  = 100\n",
        "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)\n",
        "\n",
        "'''\n",
        "Alright class, our model is designed and compiled.\n",
        "It's time to start the learning process.\n",
        "This is where we show the model our training data and let it figure out the patterns. This is done with the .fit() method.\n",
        "\n",
        "\n",
        "Let's break down the arguments:\n",
        "X_train, y_train: This is the most important part.\n",
        " We are providing the model with our training features (X_train) and the corresponding correct answers (y_train).\n",
        " The model will look at X_train, make a prediction, compare it to y_train, calculate the loss,\n",
        " and then use the optimizer to update its internal weights to do better next time.\n",
        "\n",
        "\n",
        "batch_size = 32: The model doesn't look at all 8,000 training samples at once.\n",
        "That would be computationally expensive. Instead, it looks at them in small groups or 'batches'.\n",
        "Here, it will take the first 32 customers, make predictions, calculate the average loss for that batch,\n",
        "and update its weights. Then it takes the next 32, and so on, until it has gone through all 8,000 training samples.\n",
        "32 is a common and efficient default value.\n",
        "\n",
        "\n",
        "epochs = 100: An epoch is one full pass through the entire training dataset.\n",
        "We've told our model to go through all 8,000 training samples (in batches of 32) a total of 100 times.\n",
        "Why multiple epochs? One pass is not enough for the network to learn the complex patterns.\n",
        "By repeatedly seeing the data, the optimizer can gradually fine-tune the weights, getting closer and closer to the best solution.\n",
        "As you can see from the output, for each of the 100 epochs, the model reports its progress.\n",
        "You will generally see the loss go down and the accuracy go up as the training progresses,\n",
        "which tells us that the model is learning successfully.\"\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qNQ3WevnJAbS",
        "outputId": "816999d1-e886-47c4-aeda-ca6f1938e6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7629 - loss: 0.5751\n",
            "Epoch 2/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7941 - loss: 0.4592\n",
            "Epoch 3/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7997 - loss: 0.4435\n",
            "Epoch 4/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8118 - loss: 0.4271\n",
            "Epoch 5/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8135 - loss: 0.4226\n",
            "Epoch 6/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8256 - loss: 0.4182\n",
            "Epoch 7/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8251 - loss: 0.4115\n",
            "Epoch 8/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8284 - loss: 0.4040\n",
            "Epoch 9/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8335 - loss: 0.4003\n",
            "Epoch 10/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8313 - loss: 0.4004\n",
            "Epoch 11/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8329 - loss: 0.4008\n",
            "Epoch 12/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8331 - loss: 0.4036\n",
            "Epoch 13/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8247 - loss: 0.4080\n",
            "Epoch 14/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8330 - loss: 0.3957\n",
            "Epoch 15/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8359 - loss: 0.3995\n",
            "Epoch 16/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8340 - loss: 0.4002\n",
            "Epoch 17/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8244 - loss: 0.4067\n",
            "Epoch 18/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8321 - loss: 0.3947\n",
            "Epoch 19/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4033\n",
            "Epoch 20/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8320 - loss: 0.3997\n",
            "Epoch 21/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8413 - loss: 0.3899\n",
            "Epoch 22/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8436 - loss: 0.3895\n",
            "Epoch 23/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8370 - loss: 0.3977\n",
            "Epoch 24/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8361 - loss: 0.3933\n",
            "Epoch 25/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8410 - loss: 0.3971\n",
            "Epoch 26/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8376 - loss: 0.4025\n",
            "Epoch 27/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8492 - loss: 0.3735\n",
            "Epoch 28/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8439 - loss: 0.3819\n",
            "Epoch 29/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8426 - loss: 0.3935\n",
            "Epoch 30/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8371 - loss: 0.3989\n",
            "Epoch 31/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8456 - loss: 0.3747\n",
            "Epoch 32/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8414 - loss: 0.3898\n",
            "Epoch 33/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8420 - loss: 0.3862\n",
            "Epoch 34/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8509 - loss: 0.3765\n",
            "Epoch 35/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8487 - loss: 0.3851\n",
            "Epoch 36/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8516 - loss: 0.3775\n",
            "Epoch 37/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8444 - loss: 0.3838\n",
            "Epoch 38/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8447 - loss: 0.3874\n",
            "Epoch 39/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8455 - loss: 0.3847\n",
            "Epoch 40/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8553 - loss: 0.3719\n",
            "Epoch 41/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8473 - loss: 0.3805\n",
            "Epoch 42/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8357 - loss: 0.3968\n",
            "Epoch 43/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8466 - loss: 0.3741\n",
            "Epoch 44/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8455 - loss: 0.3829\n",
            "Epoch 45/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8482 - loss: 0.3739\n",
            "Epoch 46/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8561 - loss: 0.3630\n",
            "Epoch 47/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8567 - loss: 0.3599\n",
            "Epoch 48/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8586 - loss: 0.3514\n",
            "Epoch 49/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8509 - loss: 0.3654\n",
            "Epoch 50/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8582 - loss: 0.3551\n",
            "Epoch 51/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8529 - loss: 0.3594\n",
            "Epoch 52/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8615 - loss: 0.3417\n",
            "Epoch 53/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8594 - loss: 0.3413\n",
            "Epoch 54/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8555 - loss: 0.3561\n",
            "Epoch 55/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8567 - loss: 0.3538\n",
            "Epoch 56/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8687 - loss: 0.3299\n",
            "Epoch 57/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8682 - loss: 0.3310\n",
            "Epoch 58/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8574 - loss: 0.3419\n",
            "Epoch 59/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8567 - loss: 0.3436\n",
            "Epoch 60/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8613 - loss: 0.3431\n",
            "Epoch 61/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8660 - loss: 0.3351\n",
            "Epoch 62/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8556 - loss: 0.3498\n",
            "Epoch 63/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8619 - loss: 0.3376\n",
            "Epoch 64/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8625 - loss: 0.3354\n",
            "Epoch 65/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8661 - loss: 0.3287\n",
            "Epoch 66/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8662 - loss: 0.3278\n",
            "Epoch 67/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8637 - loss: 0.3376\n",
            "Epoch 68/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8614 - loss: 0.3389\n",
            "Epoch 69/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8642 - loss: 0.3278\n",
            "Epoch 70/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8632 - loss: 0.3340\n",
            "Epoch 71/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8625 - loss: 0.3401\n",
            "Epoch 72/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3350\n",
            "Epoch 73/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8653 - loss: 0.3279\n",
            "Epoch 74/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8639 - loss: 0.3379\n",
            "Epoch 75/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8692 - loss: 0.3309\n",
            "Epoch 76/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8577 - loss: 0.3365\n",
            "Epoch 77/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3319\n",
            "Epoch 78/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8608 - loss: 0.3369\n",
            "Epoch 79/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8691 - loss: 0.3230\n",
            "Epoch 80/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8645 - loss: 0.3284\n",
            "Epoch 81/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8596 - loss: 0.3380\n",
            "Epoch 82/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8664 - loss: 0.3246\n",
            "Epoch 83/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8533 - loss: 0.3410\n",
            "Epoch 84/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8627 - loss: 0.3424\n",
            "Epoch 85/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8622 - loss: 0.3316\n",
            "Epoch 86/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8649 - loss: 0.3325\n",
            "Epoch 87/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8621 - loss: 0.3329\n",
            "Epoch 88/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8607 - loss: 0.3328\n",
            "Epoch 89/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8600 - loss: 0.3415\n",
            "Epoch 90/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8696 - loss: 0.3241\n",
            "Epoch 91/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8633 - loss: 0.3367\n",
            "Epoch 92/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8647 - loss: 0.3308\n",
            "Epoch 93/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8678 - loss: 0.3243\n",
            "Epoch 94/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8621 - loss: 0.3302\n",
            "Epoch 95/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8597 - loss: 0.3377\n",
            "Epoch 96/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8652 - loss: 0.3334\n",
            "Epoch 97/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8703 - loss: 0.3124\n",
            "Epoch 98/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8616 - loss: 0.3347\n",
            "Epoch 99/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8691 - loss: 0.3231\n",
            "Epoch 100/100\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8674 - loss: 0.3278\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAlright class, our model is designed and compiled.\\nIt\\'s time to start the learning process.\\nThis is where we show the model our training data and let it figure out the patterns. This is done with the .fit() method.\\n\\n\\nLet\\'s break down the arguments:\\nX_train, y_train: This is the most important part.\\n We are providing the model with our training features (X_train) and the corresponding correct answers (y_train).\\n The model will look at X_train, make a prediction, compare it to y_train, calculate the loss,\\n and then use the optimizer to update its internal weights to do better next time.\\n\\n\\nbatch_size = 32: The model doesn\\'t look at all 8,000 training samples at once.\\nThat would be computationally expensive. Instead, it looks at them in small groups or \\'batches\\'.\\nHere, it will take the first 32 customers, make predictions, calculate the average loss for that batch,\\nand update its weights. Then it takes the next 32, and so on, until it has gone through all 8,000 training samples.\\n32 is a common and efficient default value.\\n\\n\\nepochs = 100: An epoch is one full pass through the entire training dataset.\\nWe\\'ve told our model to go through all 8,000 training samples (in batches of 32) a total of 100 times.\\nWhy multiple epochs? One pass is not enough for the network to learn the complex patterns.\\nBy repeatedly seeing the data, the optimizer can gradually fine-tune the weights, getting closer and closer to the best solution.\\nAs you can see from the output, for each of the 100 epochs, the model reports its progress.\\nYou will generally see the loss go down and the accuracy go up as the training progresses,\\nwhich tells us that the model is learning successfully.\"\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FurKftGJJAd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------- Evaluating the Model ---------------------#\n",
        "# the goal is to use this ANN model to predict the probability of the customer leaving the bank\n",
        "# Predicting the churn probability for single observations\n",
        "\n",
        "#Geography: French\n",
        "#Credit Score:600\n",
        "#Gender: Male\n",
        "#Age: 40 years old\n",
        "#Tenure: 3 years\n",
        "#Balance: $60000\n",
        "#Number of Products: 2\n",
        "#with Credit Card\n",
        "#Active member\n",
        "#Estimated Salary: $50000\n",
        "\n",
        "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
        "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)\n",
        "# this customer has 4% chance to leave the bank\n",
        "\n",
        "\n",
        "'''\n",
        "\"Now that our model is trained, let's use it for its intended purpose:\n",
        " making a prediction on a new, single customer. This shows us how the bank could use this model in a real-world scenario\n",
        "\n",
        "We've defined a hypothetical customer in the comments.\n",
        "Our first step is to translate these characteristics into the numerical array format the model understands.\n",
        "This includes the one-hot encoding for 'France' (1, 0, 0) and the label encoding for 'Male' (1).\n",
        "Look at the first print statement:\n",
        "\n",
        "print(ann.predict(sc.transform([[...]])))\n",
        "First, we must pass this new customer's data through our scaler using sc.transform().\n",
        "Every new piece of data must go through the exact same preprocessing steps as our training data.\n",
        "Then, we feed this scaled data into ann.predict().\n",
        "\n",
        "The output, [[0.034...]], is the raw probability from our sigmoid output neuron.\n",
        "This means our model predicts there is a 3.4% chance this specific customer will leave the bank.\n",
        "Now for the second print statement:\n",
        "\n",
        "print(ann.predict(sc.transform([[...]])) > 0.5)\n",
        "Often, we don't just want a probability; we want a definite 'Yes' or 'No' decision.\n",
        "We set a threshold, typically 50% (or 0.5). If the predicted probability is greater than 0.5,\n",
        "we classify the customer as 'Exited'. If not, they are classified as 'Stays'.\n",
        "Since 3.4% is much less than 50%, the expression evaluates to False.\n",
        "The model's final verdict is that this customer will not leave the bank.\n",
        "This is a practical example of how to turn the model's output into an actionable business insight.\"\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "fy19IiF0J0Ym",
        "outputId": "dc64755f-71a3-4087-9d34-081ba67c88f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "[[0.03989722]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "[[False]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"Now that our model is trained, let\\'s use it for its intended purpose:\\n making a prediction on a new, single customer. This shows us how the bank could use this model in a real-world scenario\\n\\nWe\\'ve defined a hypothetical customer in the comments.\\nOur first step is to translate these characteristics into the numerical array format the model understands.\\nThis includes the one-hot encoding for \\'France\\' (1, 0, 0) and the label encoding for \\'Male\\' (1).\\nLook at the first print statement:\\n\\nprint(ann.predict(sc.transform([[...]])))\\nFirst, we must pass this new customer\\'s data through our scaler using sc.transform().\\nEvery new piece of data must go through the exact same preprocessing steps as our training data.\\nThen, we feed this scaled data into ann.predict().\\n\\nThe output, [[0.034...]], is the raw probability from our sigmoid output neuron.\\nThis means our model predicts there is a 3.4% chance this specific customer will leave the bank.\\nNow for the second print statement:\\n\\nprint(ann.predict(sc.transform([[...]])) > 0.5)\\nOften, we don\\'t just want a probability; we want a definite \\'Yes\\' or \\'No\\' decision.\\nWe set a threshold, typically 50% (or 0.5). If the predicted probability is greater than 0.5,\\nwe classify the customer as \\'Exited\\'. If not, they are classified as \\'Stays\\'.\\nSince 3.4% is much less than 50%, the expression evaluates to False.\\nThe model\\'s final verdict is that this customer will not leave the bank.\\nThis is a practical example of how to turn the model\\'s output into an actionable business insight.\"\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show the vector of predictions and real values\n",
        "#probabilities\n",
        "y_pred_prob = ann.predict(X_test)\n",
        "\n",
        "\n",
        "'''\n",
        "\"Predicting for one customer is useful, but to truly know how good our model is,\n",
        "we need to evaluate it on the entire unseen test set. Remember, this is the data our model has never been exposed to before.\n",
        "\n",
        "y_pred_prob = ann.predict(X_test):\n",
        "Here, instead of passing in one customer, we're passing in the entire X_test dataset,\n",
        "which contains the features for our 2,000 test customers.\n",
        "\n",
        "The model will rapidly make a prediction for every single one of them.\n",
        "\n",
        "The result, which we store in y_pred_prob, is a vector containing 2,000 probability values—one for each customer in the test set.\n",
        "\n",
        "This gives us the raw probabilistic output for our entire test set, which we will use in the next steps\n",
        "to calculate our final performance metrics.\"\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "-2A2oV7FJ0bF",
        "outputId": "93845ae6-7eca-4a1b-b5d3-1bd203c6ce16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"Predicting for one customer is useful, but to truly know how good our model is,\\nwe need to evaluate it on the entire unseen test set. Remember, this is the data our model has never been exposed to before.\\n\\ny_pred_prob = ann.predict(X_test):\\nHere, instead of passing in one customer, we\\'re passing in the entire X_test dataset,\\nwhich contains the features for our 2,000 test customers.\\n\\nThe model will rapidly make a prediction for every single one of them.\\n\\nThe result, which we store in y_pred_prob, is a vector containing 2,000 probability values—one for each customer in the test set.\\n\\nThis gives us the raw probabilistic output for our entire test set, which we will use in the next steps\\nto calculate our final performance metrics.\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#probabilities to binary\n",
        "y_pred = (y_pred_prob > 0.5)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)), 1))\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\"We have our list of predicted probabilities, but to calculate things like accuracy,\n",
        "we need a final binary prediction: 0 (Stays) or 1 (Exits).\n",
        "\n",
        "y_pred = (y_pred_prob > 0.5): This is the same logic we used for the single customer,\n",
        "but now we're applying it to our entire array of 2,000 predicted probabilities.\n",
        "For every probability in y_pred_prob, this line checks if it is greater than 0.5.\n",
        "\n",
        "The result, y_pred, is a new array of True and False values. (In Python, True is treated as 1 and False is treated as 0).\n",
        "This is our final set of predictions.\n",
        "\n",
        "print(np.concatenate(...)): The second line is just for visualization.\n",
        "It's a handy way to compare our model's predictions with the actual results side-by-side.\n",
        "\n",
        "np.concatenate is a NumPy function that joins arrays together.\n",
        "We are taking our predictions (y_pred) and the true answers (y_test) and stacking them into a two-column list.\n",
        "The .reshape(...) part is just to make sure both arrays are arranged as vertical columns before they are joined.\n",
        "The output shows this comparison clearly. The first column is our model's prediction,\n",
        "and the second column is the ground truth. For example, in the second row [0 1], our model predicted 0 (Stays),\n",
        "but the customer actually 1 (Exited). This was an incorrect prediction. The other rows show correct predictions.\"\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "nXWbou0YJ0dV",
        "outputId": "6fd90631-0740-435d-f816-569e17995cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " ...\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"We have our list of predicted probabilities, but to calculate things like accuracy,\\nwe need a final binary prediction: 0 (Stays) or 1 (Exits).\\n\\ny_pred = (y_pred_prob > 0.5): This is the same logic we used for the single customer,\\nbut now we\\'re applying it to our entire array of 2,000 predicted probabilities.\\nFor every probability in y_pred_prob, this line checks if it is greater than 0.5.\\n\\nThe result, y_pred, is a new array of True and False values. (In Python, True is treated as 1 and False is treated as 0).\\nThis is our final set of predictions.\\n\\nprint(np.concatenate(...)): The second line is just for visualization.\\nIt\\'s a handy way to compare our model\\'s predictions with the actual results side-by-side.\\n\\nnp.concatenate is a NumPy function that joins arrays together.\\nWe are taking our predictions (y_pred) and the true answers (y_test) and stacking them into a two-column list.\\nThe .reshape(...) part is just to make sure both arrays are arranged as vertical columns before they are joined.\\nThe output shows this comparison clearly. The first column is our model\\'s prediction,\\nand the second column is the ground truth. For example, in the second row [0 1], our model predicted 0 (Stays),\\nbut the customer actually 1 (Exited). This was an incorrect prediction. The other rows show correct predictions.\"\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix\", confusion_matrix)\n",
        "print(\"Accuracy Score\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "'''\n",
        "This is the simplest metric. It answers the question: \"Out of all predictions, what percentage did the model get right?\"\n",
        "\n",
        "The result 0.865 means our model was correct on 86.5% of the 2,000 customers in the unseen test set. This is a very strong result.\n",
        "\n",
        "Next, and more importantly, the Confusion Matrix:\n",
        "\n",
        "print(\"Confusion Matrix\", confusion_matrix)\n",
        "\n",
        "Accuracy can sometimes be misleading, especially if one class is much more common than the other.\n",
        "A confusion matrix gives us a much richer understanding of our model's performance by breaking down the correct and incorrect predictions for each class.\n",
        "Let's interpret the matrix: [[1508, 87], [183, 222]]\n",
        "\n",
        "Top-Left (1502): True Negatives. These are the customers who did not leave, and our model correctly predicted they would not leave.\n",
        " This is the biggest group and a good result.\n",
        "Top-Right (93): False Positives. These are customers who did not leave, but our model incorrectly predicted they would leave.\n",
        "(The model was 'falsely positive' about them leaving).\n",
        "Bottom-Left (189): False Negatives. This is often the most important number for business.\n",
        "These are customers who did leave, but our model incorrectly predicted they would stay.\n",
        "These are the churners we failed to identify.\n",
        "Bottom-Right (216): True Positives. These are customers who did leave, and our model correctly predicted they would leave.\n",
        "These are the successes where the bank could now intervene.\n",
        "By combining the accuracy score with the detailed insights from the confusion matrix,\n",
        "we get a complete picture of our model's strengths and weaknesses.\"\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Ye0CFp46J0fo",
        "outputId": "2934cf0e-1449-4783-e77c-70f1b11ae735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'numpy.ndarray' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3034562194.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion Matrix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy Score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9ewLWX5J0h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zq5t3C7RJ0kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkX4SE-3J0mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whNQnG4hJ0tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9H0JhypJAgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bzlIBLM8JAj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}